---
title: "ELA V4"
output: pdf_document
date: "2025-08-13"
---

```{r}


set.seed(42)
library(stats)  # for optim



# vectorised uniform respecting per-gene bounds
runif_vec <- function(lower, upper) {
  stopifnot(length(lower) == length(upper))
  lower + runif(length(lower)) * (upper - lower)
}
# Created a function which picks a random point inside a multidimensional box, where each dimension essentially has its own upper and lower limits. It is basically a “random coordinate generator” within given boundaries.

# softmax with numerical stability 
softmax <- function(x, tau = 1) {
  if (!is.finite(tau) || tau <= 0) stop("tau must be > 0")
  if (all(is.na(x))) return(rep(1/length(x), length(x)))
  x_clean <- x[is.finite(x)]
  if (length(x_clean) == 0) return(rep(1/length(x), length(x)))
  z <- (x - max(x, na.rm = TRUE)) / tau
  z <- pmax(z, -500)
  ez <- exp(z)
  s  <- sum(ez, na.rm = TRUE)
  if (!is.finite(s) || s == 0) rep(1/length(x), length(x)) else ez / s
}
# -	Used softmax function to convert raw fitness scores into selection probabilities. This is important as this is how “fitness” translates to “survival probability”. Use of tau parameter was to control selection pressure. Low tau -> only best individuals reproduce. High tau -> more democratic selection. This is where “survival of the fittest” gets mathematically implemented.


# clamping with vectorization
clamp <- function(x, lo, hi) {
  if (length(lo) == 1) lo <- rep(lo, length(x))
  if (length(hi) == 1) hi <- rep(hi, length(x))
  pmin(pmax(x, lo), hi)
}
# squeezes values so they cant go below a minimum or above a maximum. If they try, they get pulled back inside the allowed range.

# population diversity 
pop_diversity <- function(P) {
  if (ncol(P) <= 1) return(0)
  centroid  <- rowMeans(P, na.rm = TRUE)
  distances <- sqrt(colSums((P - centroid)^2, na.rm = TRUE))
  mean(distances, na.rm = TRUE)
}
# P = the population, stored as a matrix (each column is an individual, each row is a parameter).
# If there’s only 1 individual, return 0 because diversity can’t exist with a single point.
# centroid <- calculate the centroid (average position of the population in parameter space). 
# distances <- compute the distance of each individual from the centroid.
# average of those distances -> diversity.
# essentially measures how spread out are the individuals in the population


# population convergence
pop_convergence <- function(P) {
  if (ncol(P) <= 1) return(0)
  param_std <- apply(P, 1, sd, na.rm = TRUE)
  mean(param_std, na.rm = TRUE)
}
# If only 1 individual, return 0 (no convergence information possible)
# for each parameter(each row), we calculate standard deviation across individuals, then take average
# measuring how much individuals agree on parameter values
# Low value = everyone is clustering together (high convergence).
# High value = still spread out (low convergence).

# population stats
pop_stats <- function(P, f) {
  list(
    fitness = c(best = max(f, na.rm = TRUE), 
                mean = mean(f, na.rm = TRUE), 
                median = median(f, na.rm = TRUE),
                std = sd(f, na.rm = TRUE),
                q25 = quantile(f, 0.25, na.rm = TRUE),
                q75 = quantile(f, 0.75, na.rm = TRUE)),
    diversity = pop_diversity(P),
    convergence = pop_convergence(P)
  )
}
# inputs, P - population, f - fitness value for each individual
# we are summarising fitness statistics
# then call population diversity and convergence
# This gives us a summary of how well the population is doing and how it’s behaving.


# parameter validation
validate_ea_params <- function(fitness_fn, dim, lower, upper, pop_size, gens, 
                               pcross, elitism, patience, eval_budget) {
  stopifnot(
    is.function(fitness_fn),
    is.numeric(dim) && length(dim) == 1 && dim > 0,
    is.numeric(pop_size) && pop_size > 0,
    is.numeric(gens) && gens > 0,
    is.numeric(pcross) && pcross >= 0 && pcross <= 1,
    is.numeric(elitism) && elitism >= 0 && elitism < pop_size,
    is.numeric(patience) && patience > 0,
    is.numeric(eval_budget) && eval_budget > 0
  )
  lower <- rep(lower, length.out = dim)
  upper <- rep(upper, length.out = dim)
  if (any(upper <= lower)) stop("All upper bounds must be > lower bounds")
  
  test_point <- runif_vec(lower, upper)
  test_fitness <- tryCatch(
    fitness_fn(test_point),
    error = function(e) stop(paste("Fitness function error:", e$message))
  )
  if (!is.numeric(test_fitness) || length(test_fitness) != 1 || !is.finite(test_fitness)) {
    stop("Fitness function must return a single finite numeric value")
  }
  list(lower = lower, upper = upper)
}
# important safety check before EA.
# use of stopifnot() makes sure our inputs make sense: fitness function is valid, dimensions are positive, population size, generations, crossover rate, elitism, patience, eval budget all within valid ranges
# we expand upper and lower bounds so each dimension has its own unique limit
# we check that every upper bound is bigger than its lower bound.
# Test the fitness function:
# Generate a random test point (runif_vec).
# Pass it into the fitness function.
# Make sure it returns a single number that is finite.
# return the validated bounds.
# essentially before starting the evolutionary run, make sure the setup isn’t broken: parameters make sense, bounds are valid, and the fitness function actually works.

# selection

select_parent <- function(f, 
                          mode = c("proportionate", "tournament", "rank"), 
                          k = 3,           # tournament size (how many compete)
                          tau = 1) {       # temperature for softmax (proportionate)
  # Pick which selection rule to use (ensures a valid choice)
  mode <- match.arg(mode)
  
  # Number of individuals in the population (one fitness per individual)
  n <- length(f)
  
  # If there is only one candidate, return it (index 1)
  if (n == 1) return(1)
  
  # Apply the chosen selection strategy
  switch(
    mode,
    
    # Proportionate selection (a.k.a. fitness-proportional):
    # Individuals with higher fitness get higher probability,
    # implemented with a numerically stable softmax.
    "proportionate" = {
      # Shift fitnesses so the largest becomes ~0 (prevents overflow in exp)
      f_shift <- f - max(f, na.rm = TRUE)
      # Convert shifted fitness to probabilities; tau controls how "peaked" they are
      probs <- softmax(f_shift, tau = tau)
      # Draw 1 parent index using those probabilities
      sample.int(n, 1, prob = probs)
    },
    
    # Tournament selection:
    # Randomly pick k individuals and select the best among them.
    "tournament" = {
      # k cannot be larger than population size
      k <- min(k, n)
      # Randomly choose k competitors (by index)
      idx <- sample.int(n, k)
      # Return the index of the competitor with the highest fitness
      idx[which.max(f[idx])]
    },
    
    # Rank-based selection:
    # Convert raw fitness to ranks (1..n), then sample proportionally to rank.
    # This reduces sensitivity to outliers in raw fitness values.
    "rank" = {
      # Break ties randomly so identical fitnesses don't bias selection
      ranks <- rank(f, ties.method = "random")
      # Probability proportional to rank (higher rank = higher prob)
      probs <- ranks / sum(ranks)
      # Sample 1 parent index using rank-based probabilities
      sample.int(n, 1, prob = probs)
    }
  )
}



# Crossover Operators

# Single-point crossover

crossover_single_point <- function(pa, pb) {
  # pa, pb: parent vectors (same length)
  d <- length(pa)
  
  # Special case: in 1 dimension, "crossover" is just averaging the two numbers
  if (d == 1L) return((pa + pb) / 2)
  
  # Choose a cut position between 1 and d-1 (so both sides are non-empty)
  q <- sample.int(d - 1, 1)
  
  # Child gets the head of parent A and the tail of parent B
  c(pa[1:q], pb[(q + 1):d])
}



# BLX-α crossover (classic)
# draw each gene from an interval that extends α beyond the parents' range.
# encourages exploration just outside the parental segment.

crossover_blx <- function(pa, pb, alpha = 0.5) {
  # For each gene, compute the min/max between the two parents
  lo <- pmin(pa, pb)
  hi <- pmax(pa, pb)
  
  # I = interval length for each gene
  I  <- hi - lo
  
  # Sample uniformly from [lo - αI, hi + αI] for each gene
  # example: a little beyond the parental range on both sides
  runif(length(pa), lo - alpha * I, hi + alpha * I)
}



# SBX (Simulated Binary Crossover)
# Produces children around/between parents with a controllable spread (eta_c).
# Larger eta_c => children closer to parents (less exploratory).

crossover_sbx <- function(pa, pb, lower, upper, eta_c = 10) {
  # pa, pb: parent vectors
  # lower, upper: per-gene bounds to clamp the child into
  d <- length(pa)
  
  # u ~ Uniform(0,1) controls sampling for each gene
  u <- runif(d)
  
  # Compute "beta" which determines how far the child can be from the parents
  # Two formulas depending on u ensure a symmetric distribution
  beta <- ifelse(
    u <= 0.5,
    (2 * u)^(1 / (eta_c + 1)),
    (1 / (2 * (1 - u)))^(1 / (eta_c + 1))
  )
  
  # Two symmetric child candidates (c1, c2)
  c1 <- 0.5 * ((1 + beta) * pa + (1 - beta) * pb)
  c2 <- 0.5 * ((1 - beta) * pa + (1 + beta) * pb)
  
  # Randomly choose one of the two children
  child <- if (runif(1) < 0.5) c1 else c2
  
  # Ensure the child respects the allowed bounds
  clamp(child, lower, upper)
}


# mutation
mutate_gaussian <- function(x, sigma_vec, pm = NULL) {
  d <- length(x)
  if (is.null(pm)) pm <- 1 / d
  mask <- runif(d) < pm
  if (any(mask)) x[mask] <- x[mask] + rnorm(sum(mask), 0, sigma_vec[mask])
  x
}

mutate_poly <- function(x, lower, upper, eta_m = 20, pm = NULL) {
  d <- length(x)
  if (is.null(pm)) pm <- 1 / d
  lo <- rep(lower, length.out = d)
  hi <- rep(upper, length.out = d)
  rng <- hi - lo
  if (any(rng <= 0)) stop("upper must be > lower for all genes")
  
  y <- (x - lo) / rng
  mask <- runif(d) < pm
  if (!any(mask)) return(x)
  
  u <- runif(d)
  mut_pow <- 1 / (eta_m + 1)
  delta1 <- y
  delta2 <- 1 - y
  deltaq <- numeric(d)
  
  idx <- which(mask)
  for (i in idx) {
    if (u[i] <= 0.5) {
      xy <- 1 - delta1[i]
      val <- 2 * u[i] + (1 - 2 * u[i]) * (xy^(eta_m + 1))
      deltaq[i] <- (val^mut_pow) - 1
    } else {
      xy <- 1 - delta2[i]
      val <- 2 * (1 - u[i]) + 2 * (u[i] - 0.5) * (xy^(eta_m + 1))
      deltaq[i] <- 1 - (val^mut_pow)
    }
  }
  y_new <- clamp(y + deltaq, 0, 1)
  out <- lo + y_new * rng
  out[!mask] <- x[!mask]
  clamp(out, lo, hi)
}

# main EA (with history tracking)
evolve_real_enhanced <- function(
  fitness_fn, dim, lower, upper,
  pop_size = 100, gens = 200,
  selection = c("tournament", "proportionate", "rank"),
  tournament_k = 3, tau = 1,
  crossover = c("sbx", "blx", "single"),
  alpha = 0.5, eta_c = 10, pcross = 0.9,
  mutation = c("poly", "gaussian"),
  mut_sd0 = 0.2, mut_sd_min = 1e-3, mut_sd_decay = 0.95,
  eta_m = 20, pmutation = NULL,
  elitism = 2, beta_replace = 0.0,
  eval_budget = Inf, patience = 30, min_delta = 1e-8,
  log_freq = 10, verbose = TRUE, seed = NULL,
  track_evals = TRUE
) {
  if (!is.null(seed)) {
    old_seed <- NULL
    if (exists(".Random.seed", .GlobalEnv)) old_seed <- .Random.seed
    on.exit({ if (!is.null(old_seed)) .Random.seed <<- old_seed }, add = TRUE)
    set.seed(seed)
  }
  
  selection <- match.arg(selection)
  crossover  <- match.arg(crossover)
  mutation   <- match.arg(mutation)
  
  bounds <- validate_ea_params(fitness_fn, dim, lower, upper, pop_size, gens,
                               pcross, elitism, patience, eval_budget)
  lower <- bounds$lower; upper <- bounds$upper; range <- upper - lower
  
  # init population
  P <- sapply(1:pop_size, function(i) runif_vec(lower, upper))
  f <- apply(P, 2, fitness_fn)
  if (!all(is.finite(f))) stop("Non-finite fitness values at initialization")
  evals <- pop_size
  
  # tracking
  best_idx <- which.max(f)
  best <- list(f = f[best_idx], theta = P[, best_idx], generation = 0)
  
  # Initialize eval tracking
  if (track_evals) {
    eval_history <- data.frame(
      evaluations = 1:pop_size,
      best_so_far = cummax(f)
    )
  }
  
  hist <- data.frame(
    generation = integer(gens + 1),
    best = numeric(gens + 1),
    mean = numeric(gens + 1),
    median = numeric(gens + 1),
    diversity = numeric(gens + 1),
    mut_sd = numeric(gens + 1)
  )
  
  stats <- pop_stats(P, f)
  hist[1, ] <- c(0, stats$fitness["best"], stats$fitness["mean"], 
                 stats$fitness["median"], stats$diversity, mut_sd0)
  
  if (verbose) cat(sprintf("Gen %3d | best=%.6f | mean=%.6f | diversity=%.4f\n",
                           0, best$f, stats$fitness["mean"], stats$diversity))
  
  no_improve <- 0L
  stop_reason <- "gens_exhausted"
  mut_sd <- mut_sd0
  
  for (g in 1:gens) {
    if (evals >= eval_budget) { stop_reason <- "eval_budget"; break }
    
    mut_sd <- max(mut_sd_min, mut_sd * mut_sd_decay)
    sigma_vec <- mut_sd * range
    
    elite_idx <- order(f, decreasing = TRUE)[1:elitism]
    elites    <- P[, elite_idx, drop = FALSE]
    elites_f  <- f[elite_idx]
    
    n_off <- pop_size - elitism
    Off <- matrix(NA_real_, nrow = dim, ncol = n_off)
    
    for (k in 1:n_off) {
      i <- select_parent(f, selection, tournament_k, tau)
      j <- select_parent(f, selection, tournament_k, tau)
      pa <- P[, i]; pb <- P[, j]
      
      if (runif(1) < pcross) {
        child <- switch(crossover,
                        "sbx"    = crossover_sbx(pa, pb, lower, upper, eta_c),
                        "blx"    = crossover_blx(pa, pb, alpha),
                        "single" = crossover_single_point(pa, pb))
      } else {
        child <- if (runif(1) < 0.5) pa else pb
      }
      
      child <- switch(mutation,
                      "gaussian" = mutate_gaussian(child, sigma_vec, pm = pmutation),
                      "poly"     = mutate_poly(child, lower, upper, eta_m = eta_m, pm = pmutation))
      
      if (runif(1) < beta_replace) {
        child <- runif_vec(lower, upper)
      }
      
      Off[, k] <- clamp(child, lower, upper)
    }
    
    f_off <- apply(Off, 2, fitness_fn)
    if (!all(is.finite(f_off))) stop("Non-finite fitness in offspring")
    evals <- evals + n_off
    
    # Track evaluations (sequential best-so-far)
    if (track_evals) {
      last_best <- tail(eval_history$best_so_far, 1)
      for (i in 1:n_off) {
        last_best <- max(last_best, f_off[i])
        eval_history <- rbind(
          eval_history,
          data.frame(evaluations = nrow(eval_history) + 1, best_so_far = last_best)
        )
      }
    }
    
    P <- cbind(elites, Off)
    f <- c(elites_f, f_off)
    
    stats <- pop_stats(P, f)
    b <- stats$fitness["best"]
    
    hist[g + 1, ] <- c(g, b, stats$fitness["mean"],
                       stats$fitness["median"], stats$diversity, mut_sd)
    
    if (b > best$f + min_delta) {
      best <- list(f = b, theta = P[, which.max(f)], generation = g)
      no_improve <- 0L
    } else {
      no_improve <- no_improve + 1L
    }
    
    if (no_improve >= patience) { stop_reason <- "early_stopping"; break }
    
    if (verbose && (g %% log_freq == 0L)) {
      cat(sprintf("Gen %3d | best=%.6f | mean=%.6f | diversity=%.4f\n",
                  g, b, stats$fitness["mean"], stats$diversity))
    }
  }
  
  history <- hist[1:(g + 1), ]
  history$evaluations <- (0:g) * (pop_size - elitism) + pop_size
  
  result <- list(
    best_fit = best$f,
    best_theta = best$theta,
    best_generation = best$generation,
    history = history,
    final_population = P,
    final_fitness = f,
    evaluations = evals,
    stop_reason = stop_reason,
    method = "EA"
  )
  
  if (track_evals) result$eval_history <- eval_history
  
  result
}

# Additional Optimizers & Helpers


# Null-coalescing helper
`%||%` <- function(x, y) if (is.null(x)) y else x

# Standardized result entry for benchmark_optimizers()
make_result_entry <- function(method, run, res) {
  list(
    method = method,
    run = run,
    best_fit = res$best_fit,
    evaluations = res$evaluations %||% NA_integer_,
    eval_history = res$eval_history %||% NULL
  )
}

# Random Search baseline
bsrs <- function(fitness_fn, dim, lower, upper, eval_budget = 10000, 
                 verbose = FALSE, track_evals = TRUE) {
  best_f <- -Inf
  best_theta <- NULL
  lower <- rep(lower, length.out = dim)
  upper <- rep(upper, length.out = dim)
  
  eval_history <- NULL
  if (track_evals) {
    eval_history <- data.frame(
      evaluations = integer(),
      best_so_far = numeric()
    )
  }
  
  for (i in 1:eval_budget) {
    theta <- runif_vec(lower, upper)
    f <- fitness_fn(theta)
    if (f > best_f) { 
      best_f <- f
      best_theta <- theta 
    }
    if (track_evals) {
      eval_history <- rbind(eval_history,
                            data.frame(evaluations = i, best_so_far = best_f))
    }
  }
  
  result <- list(
    best_fit = best_f, 
    best_theta = best_theta, 
    method = "Random Search",
    evaluations = eval_budget
  )
  if (track_evals) result$eval_history <- eval_history
  result
}

# Particle Swarm Optimization (PSO) with inertia scheduling
pso <- function(fitness_fn, dim, lower, upper, 
                swarm_size = 30, max_iter = 200,
                w = 0.7, c1 = 1.5, c2 = 1.5,
                eval_budget = Inf, verbose = FALSE,
                track_evals = TRUE, seed = NULL) {
  
  if (!is.null(seed)) set.seed(seed)
  
  lower <- rep(lower, length.out = dim)
  upper <- rep(upper, length.out = dim)
  range <- upper - lower
  vmax <- 0.2 * range
  
  # Initialize swarm
  X <- sapply(1:swarm_size, function(i) runif_vec(lower, upper))
  V <- matrix(0, nrow = dim, ncol = swarm_size)
  f <- apply(X, 2, fitness_fn)
  evals <- swarm_size
  
  # Personal bests
  pbest <- X
  pbest_f <- f
  
  # Global best
  gbest_idx <- which.max(f)
  gbest <- X[, gbest_idx]
  gbest_f <- f[gbest_idx]
  
  eval_history <- NULL
  if (track_evals) {
    eval_history <- data.frame(
      evaluations = 1:swarm_size,
      best_so_far = cummax(f)
    )
  }
  
  w0 <- w; w_end <- 0.3
  
  for (iter in 1:max_iter) {
    if (evals >= eval_budget) break
    # linear schedule
    w <- w0 + (w_end - w0) * (iter / max_iter)
    
    # Update velocities and positions
    for (i in 1:swarm_size) {
      r1 <- runif(dim)
      r2 <- runif(dim)
      V[, i] <- w * V[, i] + 
                c1 * r1 * (pbest[, i] - X[, i]) +
                c2 * r2 * (gbest - X[, i])
      V[, i] <- clamp(V[, i], -vmax, vmax)
      X[, i] <- clamp(X[, i] + V[, i], lower, upper)
    }
    
    # Evaluate
    f <- apply(X, 2, fitness_fn)
    evals <- evals + swarm_size
    
    # Update personal bests
    better <- f > pbest_f
    pbest[, better] <- X[, better]
    pbest_f[better] <- f[better]
    
    # Update global best
    best_idx <- which.max(pbest_f)
    if (pbest_f[best_idx] > gbest_f) {
      gbest <- pbest[, best_idx]
      gbest_f <- pbest_f[best_idx]
    }
    
    if (track_evals) {
      for (i in 1:swarm_size) {
        eval_history <- rbind(eval_history,
                              data.frame(evaluations = nrow(eval_history) + 1,
                                         best_so_far = gbest_f))
      }
    }
    
    if (verbose && iter %% 10 == 0) {
      cat(sprintf("PSO Iter %3d | best=%.6f\n", iter, gbest_f))
    }
  }
  
  result <- list(
    best_fit = gbest_f,
    best_theta = gbest,
    method = "PSO",
    evaluations = evals
  )
  if (track_evals) result$eval_history <- eval_history
  result
}

# Gradient Descent with central differences + momentum (gradient ascent)
gradient_descent_fd <- function(fitness_fn, dim, lower, upper,
                                x0 = NULL, lr = 0.01, momentum = 0.9,
                                max_iter = 1000, eval_budget = Inf, verbose = FALSE,
                                track_evals = TRUE, seed = NULL) {
  
  if (!is.null(seed)) set.seed(seed)
  
  lower <- rep(lower, length.out = dim)
  upper <- rep(upper, length.out = dim)
  range <- upper - lower
  
  # Initialize
  if (is.null(x0)) x0 <- runif_vec(lower, upper)
  x <- x0
  v <- rep(0, dim)  # momentum
  
  best_x <- x
  best_f <- fitness_fn(x)
  evals <- 1
  
  eval_history <- NULL
  if (track_evals) {
    eval_history <- data.frame(
      evaluations = 1,
      best_so_far = best_f
    )
  }
  
  for (iter in 1:max_iter) {
    if (evals >= eval_budget) break
    
    # Central differences
    grad <- numeric(dim)
    for (i in 1:dim) {
      if (evals + 2 > eval_budget) break
      h <- max(1e-6, 1e-4 * range[i])
      xm <- x; xp <- x
      xm[i] <- max(lower[i], x[i] - h)
      xp[i] <- min(upper[i], x[i] + h)
      fm <- fitness_fn(xm); fp <- fitness_fn(xp)
      grad[i] <- (fp - fm) / (xp[i] - xm[i] + 1e-12)
      evals <- evals + 2
    }
    if (evals >= eval_budget) break
    
    # Update with momentum (gradient ascent)
    v <- momentum * v + lr * grad
    x <- clamp(x + v, lower, upper)
    
    # Track best
    f_current <- fitness_fn(x)
    evals <- evals + 1
    if (f_current > best_f) {
      best_f <- f_current
      best_x <- x
    }
    
    if (track_evals) {
      eval_history <- rbind(eval_history,
                            data.frame(evaluations = evals,
                                       best_so_far = best_f))
    }
    
    if (verbose && iter %% 50 == 0) {
      cat(sprintf("GD Iter %3d | current=%.6f | best=%.6f\n", 
                  iter, f_current, best_f))
    }
  }
  
  result <- list(
    best_fit = best_f,
    best_theta = best_x,
    method = "Gradient Descent (FD)",
    evaluations = evals
  )
  if (track_evals) result$eval_history <- eval_history
  result
}

# Adam optimizer for differentiable problems (gradient ascent)
adam <- function(grad_fn, x0, lr = 0.001, beta1 = 0.9, beta2 = 0.999,
                 epsilon = 1e-8, max_iter = 1000, eval_fn = NULL,
                 lower = -Inf, upper = Inf, verbose = FALSE,
                 track_evals = TRUE) {
  
  x <- x0
  m <- rep(0, length(x))  # first moment
  v <- rep(0, length(x))  # second moment
  
  best_x <- x
  best_f <- if (!is.null(eval_fn)) eval_fn(x) else NA
  
  eval_history <- NULL
  if (track_evals && !is.null(eval_fn)) {
    eval_history <- data.frame(iterations = 0, value = best_f)
  }
  
  for (t in 1:max_iter) {
    grad <- grad_fn(x)
    
    # Adam updates
    m <- beta1 * m + (1 - beta1) * grad
    v <- beta2 * v + (1 - beta2) * grad^2
    m_hat <- m / (1 - beta1^t)
    v_hat <- v / (1 - beta2^t)
    
    x <- clamp(x + lr * m_hat / (sqrt(v_hat) + epsilon), lower, upper)
    
    if (!is.null(eval_fn)) {
      f_current <- eval_fn(x)
      if (is.na(best_f) || f_current > best_f) {
        best_f <- f_current
        best_x <- x
      }
      if (track_evals) {
        eval_history <- rbind(eval_history,
                              data.frame(iterations = t, value = f_current))
      }
      if (verbose && t %% 100 == 0) {
        cat(sprintf("Adam Iter %3d | current=%.6f | best=%.6f\n",
                    t, f_current, best_f))
      }
    }
  }
  
  result <- list(
    best_theta = best_x,
    best_fit = best_f,
    method = "Adam",
    iterations = max_iter
  )
  if (track_evals && !is.null(eval_fn)) result$eval_history <- eval_history
  result
}

# Test Functions


# Rastrigin with analytic gradient (for reference)
rastrigin <- function(x) {
  d <- length(x)
  -(10 * d + sum(x^2 - 10 * cos(2 * pi * x)))
}
rastrigin_grad <- function(x) {
  -(2 * x + 20 * pi * sin(2 * pi * x))
}

# Rosenbrock (smooth, good for gradient methods)
rosenbrock <- function(x) {
  d <- length(x)
  -sum(100 * (x[2:d] - x[1:(d-1)]^2)^2 + (1 - x[1:(d-1)])^2)
}

# ML Task - 2D Classification


# Generate 2D moons dataset
make_moons <- function(n = 200, noise = 0.1, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  n1 <- floor(n / 2); n2 <- n - n1
  theta1 <- seq(0, pi, length.out = n1)
  X1 <- cbind(cos(theta1), sin(theta1))
  theta2 <- seq(0, pi, length.out = n2)
  X2 <- cbind(1 - cos(theta2), 1 - sin(theta2) - 0.5)
  X <- rbind(X1, X2)
  X <- X + matrix(rnorm(n * 2, 0, noise), ncol = 2)
  y <- c(rep(0, n1), rep(1, n2))
  list(X = X, y = y)
}

# Neural network for classification
make_nn_classifier <- function(input_dim = 2, hidden_dim = 5, output_dim = 1) {
  n_params <- input_dim * hidden_dim + hidden_dim + 
              hidden_dim * output_dim + output_dim
  
  unpack <- function(theta) {
    idx <- 1
    W1 <- matrix(theta[idx:(idx + input_dim * hidden_dim - 1)], 
                 nrow = hidden_dim, ncol = input_dim)
    idx <- idx + input_dim * hidden_dim
    b1 <- theta[idx:(idx + hidden_dim - 1)]; idx <- idx + hidden_dim
    W2 <- matrix(theta[idx:(idx + hidden_dim * output_dim - 1)],
                 nrow = output_dim, ncol = hidden_dim)
    idx <- idx + hidden_dim * output_dim
    b2 <- theta[idx:(idx + output_dim - 1)]
    list(W1 = W1, b1 = b1, W2 = W2, b2 = b2)
  }
  
  forward <- function(X, theta) {
    p <- unpack(theta); n <- nrow(X)
    Z1 <- X %*% t(p$W1) + matrix(p$b1, n, length(p$b1), byrow = TRUE)
    A1 <- tanh(Z1)
    Z2 <- A1 %*% t(p$W2) + matrix(p$b2, n, length(p$b2), byrow = TRUE)
    1 / (1 + exp(-Z2))  # sigmoid
  }
  
  loss <- function(theta, X, y) {
    pred <- forward(X, theta)
    eps <- 1e-7
    pred <- pmax(pmin(pred, 1 - eps), eps)
    -mean(y * log(pred) + (1 - y) * log(1 - pred))
  }
  
  accuracy <- function(theta, X, y) {
    pred <- forward(X, theta)
    mean((pred > 0.5) == y)
  }
  
  list(n_params = n_params, forward = forward, 
       loss = loss, accuracy = accuracy, unpack = unpack)
}

# Gradient computation for NN (backprop)
nn_gradient <- function(theta, X, y, nn) {
  n <- nrow(X)
  p <- nn$unpack(theta)
  Z1 <- X %*% t(p$W1) + matrix(p$b1, n, length(p$b1), byrow = TRUE)
  A1 <- tanh(Z1)
  Z2 <- A1 %*% t(p$W2) + matrix(p$b2, n, length(p$b2), byrow = TRUE)
  A2 <- 1 / (1 + exp(-Z2))
  dZ2 <- (A2 - y) / n
  dW2 <- t(dZ2) %*% A1
  db2 <- colSums(dZ2)
  dA1 <- dZ2 %*% p$W2
  dZ1 <- dA1 * (1 - A1^2)
  dW1 <- t(dZ1) %*% X
  db1 <- colSums(dZ1)
  c(as.vector(dW1), db1, as.vector(dW2), db2)
}


# Drone Environment


make_field <- function(J = 40, r = 0.08, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  obstacles <- cbind(runif(J, -0.5, 1), runif(J, -1, 1))
  list(obstacles = obstacles, radius = r)
}

check_status <- function(x, field, k, kmax) {
  if (x[2] >= 1) return(1)                                   # success
  if (abs(x[1]) >= 1 || x[2] <= -1 || k > kmax) return(-1)   # failure
  distances <- sqrt(colSums((t(field$obstacles) - x)^2))
  if (any(distances <= field$radius)) return(-1)             # collision
  0
}

make_controller <- function() {
  n_params <- 2*5 + 5 + 5*2 + 2
  unpack <- function(theta) {
    W1 <- matrix(theta[1:10], nrow = 5, ncol = 2)
    b1 <- theta[11:15]
    W2 <- matrix(theta[16:25], nrow = 2, ncol = 5)
    b2 <- theta[26:27]
    list(W1 = W1, b1 = b1, W2 = W2, b2 = b2)
  }
  forward <- function(x, theta) {
    p <- unpack(theta)
    h <- tanh(x %*% t(p$W1) + matrix(p$b1, nrow(x), 5, byrow = TRUE))
    y <- h %*% t(p$W2) + matrix(p$b2, nrow(x), 2, byrow = TRUE)
    pmin(pmax(y, -1), 1)
  }
  list(n_params = n_params, forward = forward, unpack = unpack)
}

simulate_game <- function(theta, controller, field, delta = 0.05, kmax = 100, x0 = NULL) {
  if (is.null(x0)) x <- c(runif(1, -0.8, 0.8), -1) else x <- x0
  traj <- matrix(NA_real_, nrow = kmax + 1, ncol = 2)
  traj[1, ] <- x
  for (k in 1:kmax) {
    u <- as.numeric(controller$forward(matrix(x, nrow = 1), theta))
    x <- x + delta * u
    traj[k + 1, ] <- x
    status <- check_status(x, field, k, kmax)
    if (status != 0) {
      return(list(
        status = status, 
        traj = traj[1:(k + 1), , drop = FALSE],
        steps = k,
        collision = (status == -1 && k < kmax)
      ))
    }
  }
  list(status = -1, traj = traj, steps = kmax, collision = FALSE)
}

make_starts <- function(T, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  cbind(runif(T, -0.8, 0.8), rep(-1, T))
}

# 1. Create the environment
field <- make_field(J = 30, r = 0.08, seed = 123)
controller <- make_controller()

# 2. Create fitness function
fitness_fn <- function(theta) {
  fitness_drone_enhanced(theta, controller, field, 
                        T = 15,  # 15 games per evaluation
                        delta = 0.05, kmax = 150)
}

# 3. Train the drone controller with EA
result <- evolve_real_enhanced(
  fitness_fn = fitness_fn,
  dim = controller$n_params,  # 27 parameters
  lower = -2, upper = 2,
  pop_size = 100, gens = 50,
  verbose = TRUE,
  seed = 42
)

# 4. Check performance
cat(sprintf("Win rate: %.1f%%\n", result$best_fit * 100))


# Test the best solution
plot_drone_trajectories <- function(theta, controller, field, n = 20) {
  plot(NA, xlim = c(-1, 1), ylim = c(-1, 1),
       xlab = "X", ylab = "Y", main = "Drone Trajectories")
  
  # Draw obstacles
  points(field$obstacles[,1], field$obstacles[,2], 
         pch = 16, col = "red", cex = 1.5)
  
  # Goal line
  abline(h = 1, col = "green", lwd = 3)
  
  # Run multiple games
  for (i in 1:n) {
    x0 <- c(runif(1, -0.8, 0.8), -1)
    game <- simulate_game(theta, controller, field, x0 = x0)
    
    # Plot trajectory
    lines(game$traj[,1], game$traj[,2], 
          col = ifelse(game$status == 1, "blue", "gray"), 
          lwd = 0.5)
  }
}

# Visualize
# Benchmark different optimizers on drone task
drone_benchmark <- function() {
  field <- make_field(J = 30, r = 0.08, seed = 123)
  controller <- make_controller()
  
  fitness_fn <- function(theta) {
    fitness_drone_enhanced(theta, controller, field, T = 10)
  }
  
  results <- benchmark_optimizers(
    fitness_fn = fitness_fn,
    dim = controller$n_params,
    lower = -2, upper = 2,
    methods = c("EA", "PSO", "Random"),
    n_runs = 10,
    eval_budget = 3000,
    verbose = TRUE
  )
  
  # Summary
  stats <- compute_statistics(results)
  print(stats)
  
  # Plot
  df <- data.frame(
    method = sapply(results, function(x) x$method),
    win_rate = sapply(results, function(x) x$best_fit)
  )
  boxplot(win_rate ~ method, data = df,
          main = "Drone Navigation Performance",
          ylab = "Win Rate")
}

# Test with metrics
test_metrics <- function(theta) {
  result <- fitness_drone_enhanced(
    theta, controller, field, 
    T = 100,  # More games for accurate stats
    delta = 0.05, kmax = 150
  )
  
  metrics <- attr(result, "metrics")
  cat(sprintf("Win rate: %.1f%%\n", metrics$win_rate * 100))
  cat(sprintf("Avg steps to goal: %.1f\n", metrics$avg_steps))
  cat(sprintf("Collision rate: %.1f%%\n", metrics$collision_rate * 100))
}




# Enhanced fitness with multiple metrics
fitness_drone_enhanced <- function(theta, controller, field, T = 10, 
                                   delta = 0.05, kmax = 100, starts = NULL) {
  if (is.null(starts)) starts <- make_starts(T)
  wins <- 0; total_steps <- 0; collisions <- 0
  for (t in 1:T) {
    result <- simulate_game(theta, controller, field, delta, kmax, x0 = starts[t, ])
    if (result$status == 1) { wins <- wins + 1; total_steps <- total_steps + result$steps }
    if (result$collision) collisions <- collisions + 1
  }
  primary_fitness <- wins / T
  avg_steps_to_goal <- if (wins > 0) total_steps / wins else kmax
  collision_rate <- collisions / T
  attr(primary_fitness, "metrics") <- list(
    win_rate = primary_fitness,
    avg_steps = avg_steps_to_goal,
    collision_rate = collision_rate
  )
  primary_fitness
}

# Benchmarking & Analysis


benchmark_optimizers <- function(
  fitness_fn, dim, lower, upper,
  methods = c("EA", "PSO", "GD", "Random"),
  n_runs = 30, eval_budget = 5000,
  verbose = TRUE, seed_base = 1000,
  ...
) {
  results <- list()
  for (run in 1:n_runs) {
    if (verbose) cat(sprintf("\n=== Run %d/%d ===\n", run, n_runs))
    run_seed <- seed_base + run
    set.seed(run_seed)
    
    # EA
    if ("EA" %in% methods) {
      if (verbose) cat("Running EA...\n")
      ea_result <- evolve_real_enhanced(
        fitness_fn = fitness_fn,
        dim = dim, lower = lower, upper = upper,
        pop_size = 50, gens = floor(eval_budget / 50),
        eval_budget = eval_budget,
        verbose = FALSE, seed = run_seed,
        track_evals = TRUE,
        ...
      )
      results[[length(results) + 1]] <- make_result_entry("EA", run, ea_result)
    }
    
    # PSO
    if ("PSO" %in% methods) {
      if (verbose) cat("Running PSO...\n")
      pso_result <- pso(
        fitness_fn = fitness_fn,
        dim = dim, lower = lower, upper = upper,
        swarm_size = 30, max_iter = floor(eval_budget / 30),
        eval_budget = eval_budget,
        verbose = FALSE, seed = run_seed,
        track_evals = TRUE
      )
      results[[length(results) + 1]] <- make_result_entry("PSO", run, pso_result)
    }
    
    # Gradient Descent
    if ("GD" %in% methods) {
      if (verbose) cat("Running Gradient Descent...\n")
      gd_result <- gradient_descent_fd(
        fitness_fn = fitness_fn,
        dim = dim, lower = lower, upper = upper,
        eval_budget = eval_budget,
        verbose = FALSE, seed = run_seed,
        track_evals = TRUE
      )
      results[[length(results) + 1]] <- make_result_entry("GD", run, gd_result)
    }
    
    # Random Search
    if ("Random" %in% methods) {
      if (verbose) cat("Running Random Search...\n")
      rs_result <- bsrs(
        fitness_fn = fitness_fn,
        dim = dim, lower = lower, upper = upper,
        eval_budget = eval_budget,
        verbose = FALSE,
        track_evals = TRUE
      )
      results[[length(results) + 1]] <- make_result_entry("Random", run, rs_result)
    }
  }
  results
}

# Statistical analysis functions
compute_statistics <- function(results) {
  df <- data.frame(
    method = sapply(results, function(x) x$method),
    run = sapply(results, function(x) x$run),
    best_fit = sapply(results, function(x) x$best_fit),
    evaluations = sapply(results, function(x) x$evaluations)
  )
  stats <- aggregate(best_fit ~ method, data = df, FUN = function(x) {
    c(mean = mean(x),
      median = median(x),
      sd = sd(x),
      min = min(x),
      max = max(x),
      q25 = quantile(x, 0.25),
      q75 = quantile(x, 0.75))
  })
  stats
}

# Cliff's Delta effect size
cliffs_delta <- function(x, y) {
  n1 <- length(x); n2 <- length(y)
  dominance <- 0
  for (i in 1:n1) for (j in 1:n2) {
    if (x[i] > y[j]) dominance <- dominance + 1
    else if (x[i] < y[j]) dominance <- dominance - 1
  }
  delta <- dominance / (n1 * n2)
  magnitude <- if (abs(delta) < 0.147) "negligible"
  else if (abs(delta) < 0.33) "small"
  else if (abs(delta) < 0.474) "medium"
  else "large"
  list(delta = delta, magnitude = magnitude)
}

# Sample efficiency plot (robust to NAs, varies by method)
plot_sample_efficiency <- function(results, title = "Sample Efficiency Comparison") {
  methods <- unique(sapply(results, `[[`, "method"))
  cols <- c("EA"="blue","PSO"="red","GD"="green","Random"="gray","Adam"="purple")
  cols <- cols[names(cols) %in% methods]
  
  xmax <- max(sapply(results, function(x) {
    if (!is.null(x$eval_history)) max(x$eval_history$evaluations) else x$evaluations
  }), na.rm = TRUE)
  ymin <- min(sapply(results, function(x) {
    if (!is.null(x$eval_history)) min(x$eval_history$best_so_far, na.rm=TRUE) else x$best_fit
  }), na.rm = TRUE)
  ymax <- max(sapply(results, function(x) {
    if (!is.null(x$eval_history)) max(x$eval_history$best_so_far, na.rm=TRUE) else x$best_fit
  }), na.rm = TRUE)
  
  plot(NA, xlim = c(0, xmax), ylim = c(ymin, ymax),
       xlab = "Evaluations", ylab = "Best-so-far Fitness", main = title)
  
  # light per-run traces
  lapply(results, function(r) {
    if (!is.null(r$eval_history)) {
      lines(r$eval_history$evaluations, r$eval_history$best_so_far,
            col = adjustcolor(cols[r$method], 0.3), lwd = 0.7)
    }
  })
  
  # mean curves
  for (m in methods) {
    rs <- results[sapply(results, function(x) x$method == m)]
    rs <- rs[!sapply(rs, function(x) is.null(x$eval_history))]
    if (length(rs) == 0) next
    all_e <- sort(unique(unlist(lapply(rs, function(x) x$eval_history$evaluations))))
    mean_curve <- sapply(all_e, function(e) {
      vals <- sapply(rs, function(x) {
        idx <- which(x$eval_history$evaluations <= e)
        if (length(idx)) x$eval_history$best_so_far[max(idx)] else NA_real_
      })
      mean(vals, na.rm = TRUE)
    })
    lines(all_e, mean_curve, col = cols[m], lwd = 2)
  }
  legend("bottomright", legend = names(cols), col = cols, lwd = 2)
}


# Ablation Studies

ablation_study <- function(fitness_fn, dim, lower, upper, 
                           n_runs = 10, eval_budget = 5000,
                           verbose = TRUE) {
  configs <- list(
    "Full EA"       = list(pcross=0.9, mutation="poly", selection="tournament"),
    "No Crossover"  = list(pcross=0.0, mutation="poly", selection="tournament"),
    "No Mutation"   = list(pcross=0.9, mutation="poly", selection="tournament", mut_sd0=0, pmutation=0),
    "Random Select" = list(pcross=0.9, mutation="poly", selection="proportionate", tau=1e10),
    "Mutation Only" = list(pcross=0.0, mutation="poly", selection="proportionate", tau=1e10)
  )

  rows <- list()
  for (cfg in names(configs)) {
    if (verbose) cat("\nTesting:", cfg, "\n")
    pars <- configs[[cfg]]

    for (run in 1:n_runs) {
      res <- evolve_real_enhanced(
        fitness_fn = fitness_fn,
        dim = dim, lower = lower, upper = upper,
        pop_size = 50, gens = floor(eval_budget / 50),
        eval_budget = eval_budget,
        pcross   = pars$pcross,
        mutation = pars$mutation,
        selection= pars$selection,
        tau      = pars$tau      %||% 1,
        mut_sd0  = pars$mut_sd0  %||% 0.2,
        pmutation= pars$pmutation%||% NULL,
        verbose  = FALSE,
        seed     = 5000 + run
      )

      rows[[length(rows) + 1]] <- data.frame(
        config = cfg, run = run, best_fit = res$best_fit
      )
    }
  }

  do.call(rbind, rows)
}

summarize_ablation <- function(raw_df) {
  aggregate(best_fit ~ config, data = raw_df, function(x)
    c(mean=mean(x), sd=sd(x), median=median(x)))
}


# Main Demo Functions

demo_rastrigin_comparison <- function() {
  cat("\n=== Rastrigin Function Benchmark ===\n")
  cat("Comparing EA, PSO, Gradient Descent, and Random Search\n")
  
  results <- benchmark_optimizers(
    fitness_fn = rastrigin,
    dim = 10,
    lower = -5.12,
    upper = 5.12,
    methods = c("EA", "PSO", "GD", "Random"),
    n_runs = 20,
    eval_budget = 5000,
    verbose = TRUE
  )
  
  # Statistics
  stats <- compute_statistics(results)
  print(stats)
  
  # Effect sizes
  cat("\n=== Effect Sizes (Cliff's Delta) ===\n")
  ea_fits <- sapply(results[sapply(results, function(x) x$method == "EA")], function(x) x$best_fit)
  for (method in c("PSO", "GD", "Random")) {
    method_fits <- sapply(results[sapply(results, function(x) x$method == method)], function(x) x$best_fit)
    effect <- cliffs_delta(ea_fits, method_fits)
    cat(sprintf("EA vs %s: δ = %.3f (%s)\n", method, effect$delta, effect$magnitude))
  }
  
  # Plots
  par(mfrow = c(1, 2))
  df <- data.frame(
    method = sapply(results, function(x) x$method),
    best_fit = sapply(results, function(x) x$best_fit)
  )
  boxplot(best_fit ~ method, data = df,
          main = "Rastrigin Optimization Results",
          ylab = "Best Fitness (higher is better)")
  plot_sample_efficiency(results, "Sample Efficiency on Rastrigin")
  par(mfrow = c(1, 1))
  
  invisible(results)
}

demo_ml_task <- function() {
  cat("\n=== ML Classification Task ===\n")
  cat("2D Moons Dataset - Neural Network Training\n")
  
  # Generate data
  data <- make_moons(n = 200, noise = 0.2, seed = 42)
  nn <- make_nn_classifier(input_dim = 2, hidden_dim = 5, output_dim = 1)
  
  # Define fitness (negative loss for maximization)
  fitness_fn <- function(theta) -nn$loss(theta, data$X, data$y)
  # Define gradient function
  grad_fn <- function(theta) -nn_gradient(theta, data$X, data$y, nn)
  
  results <- list()
  
  # EA
  cat("Training with EA...\n")
  ea_result <- evolve_real_enhanced(
    fitness_fn = fitness_fn,
    dim = nn$n_params,
    lower = -2, upper = 2,
    pop_size = 50, gens = 100,
    verbose = FALSE,
    seed = 42
  )
  ea_result$accuracy <- nn$accuracy(ea_result$best_theta, data$X, data$y)
  results$EA <- ea_result
  
  # Adam
  cat("Training with Adam...\n")
  x0 <- runif(nn$n_params, -0.5, 0.5)
  adam_result <- adam(
    grad_fn = grad_fn,
    x0 = x0,
    lr = 0.01,
    max_iter = 100 * 50,  # roughly match EA evals
    eval_fn = fitness_fn,
    lower = -2, upper = 2,
    verbose = FALSE
  )
  adam_result$accuracy <- nn$accuracy(adam_result$best_theta, data$X, data$y)
  results$Adam <- adam_result
  
  # Random Search
  cat("Training with Random Search...\n")
  rs_result <- bsrs(
    fitness_fn = fitness_fn,
    dim = nn$n_params,
    lower = -2, upper = 2,
    eval_budget = 5000,
    verbose = FALSE
  )
  rs_result$accuracy <- nn$accuracy(rs_result$best_theta, data$X, data$y)
  results$Random <- rs_result
  
  # Results
  cat("\n=== Results ===\n")
  cat(sprintf("EA:     Loss = %.4f, Accuracy = %.2f%%\n", -results$EA$best_fit, results$EA$accuracy * 100))
  cat(sprintf("Adam:   Loss = %.4f, Accuracy = %.2f%%\n", -results$Adam$best_fit, results$Adam$accuracy * 100))
  cat(sprintf("Random: Loss = %.4f, Accuracy = %.2f%%\n", -results$Random$best_fit, results$Random$accuracy * 100))
  
  # Visualization
  plot(data$X, col = data$y + 2, pch = 19,
       main = "2D Moons Classification",
       xlab = "X1", ylab = "X2")
  legend("topright", c("Class 0", "Class 1"), 
         col = c(2, 3), pch = 19)
  
  invisible(results)
}

demo_ablation <- function() {
  cat("\n=== EA Ablation Study ===\n")
  cat("Testing importance of EA components\n")
  raw <- ablation_study(
    fitness_fn = rastrigin,
    dim = 5,
    lower = -5.12,
    upper = 5.12,
    n_runs = 10,
    eval_budget = 3000,
    verbose = TRUE
  )
  print(summarize_ablation(raw))
  boxplot(best_fit ~ config, data = raw, las = 2,
          main = "EA Component Ablation", ylab = "Best Fitness (higher is better)")
  invisible(raw)
}



# Uncomment to run demos:
# demo_rastrigin_comparison()
# demo_ml_task()
# demo_ablation()
# plot_drone_trajectories(result$best_theta, controller, field)
# drone_benchmark()
# test_metrics(result$best_theta)

```


```{r}
demo_rastrigin_comparison()






```


```{r}


demo_ml_task()




```





```{r}
demo_ablation()





```

```{r}

plot_drone_trajectories(result$best_theta, controller, field)




```


```{r}
drone_benchmark()








```


```{r}
test_metrics(result$best_theta)






```